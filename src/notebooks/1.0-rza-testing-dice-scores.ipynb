{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics import Dice\n",
    "from torchmetrics.functional import dice\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import GeneralizedDiceScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MemoryEfficientSoftDiceLoss(nn.Module):\n",
    "    def __init__(self, apply_nonlin=None, batch_dice: bool = False, do_bg: bool = True, smooth: float = 1.0, ddp: bool = True):\n",
    "        \"\"\"\n",
    "        saves 1.6 GB on Dataset017 3d_lowres\n",
    "        \"\"\"\n",
    "        super(MemoryEfficientSoftDiceLoss, self).__init__()\n",
    "\n",
    "        self.do_bg = do_bg\n",
    "        self.batch_dice = batch_dice\n",
    "        self.apply_nonlin = apply_nonlin\n",
    "        self.smooth = smooth\n",
    "        self.ddp = ddp\n",
    "\n",
    "    def forward(self, x, y, loss_mask=None):\n",
    "        shp_x, shp_y = x.shape, y.shape\n",
    "\n",
    "        if self.apply_nonlin is not None:\n",
    "            x = self.apply_nonlin(x)\n",
    "\n",
    "        if not self.do_bg:\n",
    "            x = x[:, 1:]\n",
    "\n",
    "        # make everything shape (b, c)\n",
    "        axes = list(range(2, len(shp_x)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if len(shp_x) != len(shp_y):\n",
    "                y = y.view((shp_y[0], 1, *shp_y[1:]))\n",
    "\n",
    "            if all([i == j for i, j in zip(shp_x, shp_y)]):\n",
    "                # if this is the case then gt is probably already a one hot encoding\n",
    "                y_onehot = y\n",
    "            else:\n",
    "                gt = y.long()\n",
    "                y_onehot = torch.zeros(shp_x, device=x.device, dtype=torch.bool)\n",
    "                y_onehot.scatter_(1, gt, 1)\n",
    "\n",
    "            if not self.do_bg:\n",
    "                y_onehot = y_onehot[:, 1:]\n",
    "            sum_gt = y_onehot.sum(axes) if loss_mask is None else (y_onehot * loss_mask).sum(axes)\n",
    "\n",
    "        intersect = (x * y_onehot).sum(axes) if loss_mask is None else (x * y_onehot * loss_mask).sum(axes)\n",
    "        sum_pred = x.sum(axes) if loss_mask is None else (x * loss_mask).sum(axes)\n",
    "\n",
    "        # if self.ddp and self.batch_dice:\n",
    "        #    intersect = AllGatherGrad.apply(intersect)\n",
    "        #    sum_pred = AllGatherGrad.apply(sum_pred)\n",
    "        #    sum_gt = AllGatherGrad.apply(sum_gt)\n",
    "\n",
    "        if self.batch_dice:\n",
    "            intersect = intersect.sum(0)\n",
    "            sum_pred = sum_pred.sum(0)\n",
    "            sum_gt = sum_gt.sum(0)\n",
    "\n",
    "        dc = (2 * intersect) / (torch.clip(sum_gt + sum_pred + self.smooth, 1e-8))  # 2* intersect + self.smooth\n",
    "\n",
    "        dc = dc.mean()\n",
    "        return dc  # originally negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsc_loss_w = 1.0\n",
    "\n",
    "#Dice = torchmetrics.Dice(average='macro', num_classes=4)\n",
    "DiceScore = Dice()\n",
    "DiceFGScore = Dice(ignore_index=0) #ignore_index=0 means we ignore the background class\n",
    "MonaiDiceLoss = DiceLoss()\n",
    "MonaiDiceScore = GeneralizedDiceScore()\n",
    "SoftDiceLoss = MemoryEfficientSoftDiceLoss(batch_dice=False, do_bg=True, smooth=1e-5, ddp=False)\n",
    "SoftDiceLossFG = MemoryEfficientSoftDiceLoss(batch_dice=False, do_bg=False, smooth=1e-5, ddp=False)\n",
    "# nnUnetDiceLoss = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Tensors 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: tensor([[0, 1, 1],\n",
      "        [1, 2, 0],\n",
      "        [1, 2, 0]])\n",
      "Ground Truth: tensor([[1, 2, 3],\n",
      "        [0, 1, 1],\n",
      "        [0, 0, 1]])\n",
      "Probs Shape:  torch.Size([4, 3, 3])\n",
      "Preds Shape:  torch.Size([3, 3])\n",
      "Masks Shape:  torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Creating Dummy Test Tensors for testing of same format as in the model [B, C, H, W, D]\n",
    "# B = Batch Size, C = Number of Classes, H = Height, W = Width, D = Depth\n",
    "# Example with Batch Size 1, 4 Classes, Height 3, Width 3, Depth 3\n",
    "n_classes = 4\n",
    "probs = torch.tensor(\n",
    "    [[[0.7, 0.1, 0.1],  # Class 0 (corresponding to preds == 0)\n",
    "     [0.1, 0.05, 0.6],\n",
    "     [0.1, 0.05, 0.6]],\n",
    "\n",
    "    [[0.2, 0.6, 0.6],  # Class 1 (corresponding to preds == 1)\n",
    "     [0.6, 0.1, 0.2],\n",
    "     [0.6, 0.1, 0.2]],\n",
    "\n",
    "    [[0.05, 0.2, 0.2],  # Class 2 (corresponding to preds == 2)\n",
    "     [0.2, 0.7, 0.1],\n",
    "     [0.2, 0.7, 0.1]],\n",
    "\n",
    "    [[0.05, 0.1, 0.1],  # Class 3 (no predictions, so probabilities are lower)\n",
    "     [0.1, 0.15, 0.1],\n",
    "     [0.1, 0.15, 0.1]]]\n",
    ")\n",
    "preds = torch.argmax(probs, dim=0)\n",
    "masks = torch.tensor([[1, 2, 3], [0, 1, 1], [0, 0, 1]])\n",
    "\n",
    "print(f\"Preds: {preds}\")\n",
    "print(f\"Ground Truth: {masks}\")\n",
    "\n",
    "print(\"Probs Shape: \", probs.shape)\n",
    "print(\"Preds Shape: \", preds.shape)\n",
    "print(\"Masks Shape: \", masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds_oh.shape: torch.Size([1, 4, 3, 3])\n",
      "masks_oh.shape: torch.Size([1, 4, 3, 3])\n",
      "probs.shape: torch.Size([1, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding, permuting and unsqueezing to match the format BNHW[D], where B = Batch Size, N = Number of Classes, H = Height, W = Width, D = Depth\n",
    "preds_oh = torch.nn.functional.one_hot(preds, n_classes).permute(2, 0, 1).unsqueeze(0) \n",
    "masks_oh = torch.nn.functional.one_hot(masks, n_classes).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "if probs.dim() == 3:\n",
    "    probs.unsqueeze_(0)\n",
    "\n",
    "print(f\"preds_oh.shape: {preds_oh.shape}\")\n",
    "print(f\"masks_oh.shape: {masks_oh.shape}\")\n",
    "print(f\"probs.shape: {probs.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Tensors 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: tensor([[0, 0, 2],\n",
      "        [2, 2, 3],\n",
      "        [0, 2, 0]])\n",
      "ground truth: tensor([[0, 0, 2],\n",
      "        [2, 2, 0],\n",
      "        [0, 2, 2]])\n",
      "Probs Shape:  torch.Size([4, 3, 3])\n",
      "Preds Shape:  torch.Size([3, 3])\n",
      "Masks Shape:  torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Testing with example tensors from terminal\n",
    "# unique values in masks: [0 2 3]\n",
    "# unique values in preds: [0 2]\n",
    "n_classes = 4\n",
    "\n",
    "probs = torch.tensor(\n",
    "[\n",
    "    # Class 0 probabilities (corresponding to preds == 0)\n",
    "    [[0.7, 0.8, 0.1],   # Class 0 has highest probability for preds == 0\n",
    "     [0.1, 0.1, 0.1],  \n",
    "     [0.8, 0.1, 0.7]],\n",
    "\n",
    "    # Class 1 probabilities\n",
    "    [[0.1, 0.1, 0.1],   # Class 1 does not have the highest probability anywhere\n",
    "     [0.1, 0.1, 0.1],  \n",
    "     [0.05, 0.05, 0.1]],\n",
    "\n",
    "    # Class 2 probabilities (corresponding to preds == 2)\n",
    "    [[0.1, 0.05, 0.75],  # Class 2 has highest probability for preds == 2\n",
    "     [0.75, 0.8, 0.1],  \n",
    "     [0.1, 0.75, 0.1]],\n",
    "\n",
    "    # Class 3 probabilities (corresponding to preds == 3)\n",
    "    [[0.1, 0.05, 0.05],  # Class 3 has highest probability where preds == 3\n",
    "     [0.05, 0.05, 0.7],  \n",
    "     [0.05, 0.1, 0.1]]\n",
    "]\n",
    ")\n",
    "preds = torch.argmax(probs, dim=0)\n",
    "masks = torch.tensor([[0, 0, 2], [2, 2, 0], [0, 2, 2]])\n",
    "\n",
    "print(f\"preds: {preds}\")\n",
    "print(f\"ground truth: {masks}\")\n",
    "\n",
    "print(\"Probs Shape: \", probs.shape)\n",
    "print(\"Preds Shape: \", preds.shape)\n",
    "print(\"Masks Shape: \", masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds_oh.shape: torch.Size([1, 4, 3, 3])\n",
      "masks_oh.shape: torch.Size([1, 4, 3, 3])\n",
      "probs.shape: torch.Size([1, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding, permuting and unsqueezing to match the format BNHW[D], where B = Batch Size, N = Number of Classes, H = Height, W = Width, D = Depth\n",
    "preds_oh = torch.nn.functional.one_hot(preds, n_classes).permute(2, 0, 1).unsqueeze(0) \n",
    "masks_oh = torch.nn.functional.one_hot(masks, n_classes).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "if probs.dim() == 3:\n",
    "    probs.unsqueeze_(0)\n",
    "\n",
    "print(f\"preds_oh.shape: {preds_oh.shape}\")\n",
    "print(f\"masks_oh.shape: {masks_oh.shape}\")\n",
    "print(f\"probs.shape: {probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Tensor 3 (Background heavy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: tensor([[0, 0, 0],\n",
      "        [3, 2, 0],\n",
      "        [0, 1, 0]])\n",
      "ground truth: tensor([[0, 0, 0],\n",
      "        [0, 2, 3],\n",
      "        [0, 1, 0]])\n",
      "Probs Shape:  torch.Size([4, 3, 3])\n",
      "Preds Shape:  torch.Size([3, 3])\n",
      "Masks Shape:  torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "n_classes = 4\n",
    "\n",
    "probs = torch.tensor(\n",
    "[\n",
    "    # Class 0 probabilities (background)\n",
    "    [[0.9, 0.9, 0.9],  # Class 0 dominates where ground truth is 0\n",
    "     [0.1, 0.1, 0.9],\n",
    "     [0.9, 0.1, 0.9]],\n",
    "\n",
    "    # Class 1 probabilities\n",
    "    [[0.05, 0.05, 0.05],   # Low probability for Class 1 where it's not the ground truth\n",
    "     [0.05, 0.05, 0.05],\n",
    "     [0.05, 0.8, 0.05]],   # Higher probability for Class 1 at (2, 1)\n",
    "\n",
    "    # Class 2 probabilities\n",
    "    [[0.05, 0.05, 0.05],   # Low probability for Class 2 where it's not the ground truth\n",
    "     [0.1, 0.8, 0.05],     # Higher probability for Class 2 at (1, 1)\n",
    "     [0.05, 0.05, 0.05]],\n",
    "\n",
    "    # Class 3 probabilities\n",
    "    [[0.05, 0.05, 0.05],   # Low probability for Class 3 where it's not the ground truth\n",
    "     [0.8, 0.05, 0.05],    # Higher probability for Class 3 at (1, 0)\n",
    "     [0.05, 0.05, 0.05]]\n",
    "]\n",
    ")\n",
    "preds = torch.argmax(probs, dim=0)\n",
    "masks = torch.tensor([[0, 0, 0 ], [0, 2, 3], [0, 1, 0]])\n",
    "\n",
    "print(f\"preds: {preds}\")\n",
    "print(f\"ground truth: {masks}\")\n",
    "\n",
    "print(\"Probs Shape: \", probs.shape)\n",
    "print(\"Preds Shape: \", preds.shape)\n",
    "print(\"Masks Shape: \", masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds_oh.shape: torch.Size([1, 4, 3, 3])\n",
      "masks_oh.shape: torch.Size([1, 4, 3, 3])\n",
      "probs.shape: torch.Size([1, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding, permuting and unsqueezing to match the format BNHW[D], where B = Batch Size, N = Number of Classes, H = Height, W = Width, D = Depth\n",
    "preds_oh = torch.nn.functional.one_hot(preds, n_classes).permute(2, 0, 1).unsqueeze(0) \n",
    "masks_oh = torch.nn.functional.one_hot(masks, n_classes).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "if probs.dim() == 3:\n",
    "    probs.unsqueeze_(0)\n",
    "\n",
    "print(f\"preds_oh.shape: {preds_oh.shape}\")\n",
    "print(f\"masks_oh.shape: {masks_oh.shape}\")\n",
    "print(f\"probs.shape: {probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Calculation of Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8333, 1.0000, 1.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "dice_p_cls = dice(preds, masks, average=None, num_classes=n_classes) # average=None returns dice per class\n",
    "print(dice_p_cls)\n",
    "\n",
    "# -> dice for class 1 should be 1 as predictions didn't contain the class which was also absent in the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BraTs Region Scores:\n",
      "Dice Score ET:  tensor(0.7778)\n",
      "Dice FG Score ET:  tensor(0.)\n",
      "Dice Score TC:  tensor(0.7778)\n",
      "Dice FG Score TC:  tensor(0.5000)\n",
      "Dice Score WT:  tensor(0.7778)\n",
      "Dice FG Score WT:  tensor(0.6667)\n",
      "\n",
      "\n",
      "Dice Scores:\n",
      "Dice Score:  tensor(0.7778)\n",
      "Soft Dice Score: 0.43246108293533325\n",
      "Dice Score FG:  tensor(0.6667)\n",
      "Dice Score per Class:  tensor([0.8333, 1.0000, 1.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Dice Scores:\n",
    "dsc = dice(preds, masks)\n",
    "diceFG = dice(preds, masks, ignore_index=0)\n",
    "dice_p_cls = dice(preds, masks, average=None, num_classes=n_classes) # average=None returns dice per class\n",
    "\n",
    "SoftDiceScore = 1 - SoftDiceLoss(probs, masks_oh)\n",
    "\n",
    "\n",
    "# ET (Enhancing Tumor): label 3\n",
    "dice_ET = DiceScore((preds == 3), (masks == 3))\n",
    "dice_FG_ET = DiceFGScore((preds == 3), (masks == 3))\n",
    "\n",
    "# TC(Tumor Core): ET + NCR = label 1 + label 3\n",
    "dice_TC = DiceScore((preds == 1) | (preds == 3), (masks == 1) | (masks == 3))\n",
    "dice_FG_TC = DiceFGScore((preds == 1) | (preds == 3), (masks == 1) | (masks == 3))\n",
    "\n",
    "# WT (Whole Tumor): TC + ED = label 1 + label 2 + label 3\n",
    "dice_WT = DiceScore((preds > 0), (masks > 0))\n",
    "dice_FG_WT = DiceFGScore((preds > 0), (masks > 0))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"BraTs Region Scores:\")\n",
    "print(\"Dice Score ET: \", dice_ET)\n",
    "print(\"Dice FG Score ET: \", dice_FG_ET)\n",
    "print(\"Dice Score TC: \", dice_TC)\n",
    "print(\"Dice FG Score TC: \", dice_FG_TC)\n",
    "print(\"Dice Score WT: \", dice_WT)\n",
    "print(\"Dice FG Score WT: \", dice_FG_WT)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Dice Scores:\")\n",
    "print(\"Dice Score: \", dsc)\n",
    "print(f\"Soft Dice Score: {SoftDiceScore}\")\n",
    "print(\"Dice Score FG: \", diceFG)\n",
    "print(\"Dice Score per Class: \", dice_p_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Score with One-Hot Encoded Predictions and Masks:\n",
      "Dice with oh preds: 0.8888888955116272\n",
      "DiceFG with oh preds: 0.7777777910232544\n"
     ]
    }
   ],
   "source": [
    "#Using One-Hot Encoded Predictions and Masks\n",
    "print(f\"Dice Score with One-Hot Encoded Predictions and Masks:\")\n",
    "print(f\"Dice with oh preds: {DiceScore(preds_oh, masks_oh)}\")\n",
    "print(f\"DiceFG with oh preds: {DiceFGScore(preds_oh, masks_oh)}\")\n",
    "\n",
    "# Use DiceFG when using one-hot encoding and non-binary case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice ET Loss:  tensor(0.2222)\n",
      "Dice ET FG Loss: 1.0\n",
      "Dice TC Loss:  tensor(0.2222)\n",
      "Dice TC FG Loss: 0.5\n",
      "Dice WT Loss:  tensor(0.2222)\n",
      "Dice WT FG Loss: 0.3333333134651184\n",
      "Dice Loss:  tensor(0.2222)\n",
      "Soft Dice Loss: 0.5675389170646667\n",
      "Monai Dice Loss:  tensor(0.2917)\n",
      "Dice Loss per Class:  tensor([0.1667, 0.0000, 0.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "#Dice Losses\n",
    "# Brats Dice Loss\n",
    "dice_ET_loss = (1 - DiceScore((preds == 3), (masks == 3))) * dsc_loss_w\n",
    "dice_TC_loss = (1 - DiceScore((preds == 1) | (preds == 3), (masks == 1) | (masks == 3)) ) * dsc_loss_w\n",
    "dice_WT_loss = (1 - DiceScore((preds > 0), (masks > 0))) * dsc_loss_w\n",
    "\n",
    "# Brats FG Dice Loss\n",
    "diceFG_ET_loss = (1 - DiceFGScore((preds == 3), (masks == 3))) * dsc_loss_w\n",
    "diceFG_TC_loss = (1 - DiceFGScore((preds == 1) | (preds == 3), (masks == 1) | (masks == 3)) ) * dsc_loss_w\n",
    "diceFG_WT_loss = (1 - DiceFGScore((preds > 0), (masks > 0))) * dsc_loss_w\n",
    "\n",
    "dice_loss = (1-DiceScore(preds, masks)) * dsc_loss_w\n",
    "\n",
    "dice_loss_p_class = (1 - dice_p_cls) * dsc_loss_w\n",
    "\n",
    "softdice_loss = SoftDiceLoss(probs, masks_oh) * dsc_loss_w\n",
    "\n",
    "monai_dice_loss = MonaiDiceLoss(preds_oh, masks_oh) * dsc_loss_w\n",
    "\n",
    "print(\"Dice ET Loss: \", dice_ET_loss)\n",
    "print(f\"Dice ET FG Loss: {diceFG_ET_loss}\")\n",
    "print(\"Dice TC Loss: \", dice_TC_loss)\n",
    "print(f\"Dice TC FG Loss: {diceFG_TC_loss}\")\n",
    "print(\"Dice WT Loss: \", dice_WT_loss)\n",
    "print(f\"Dice WT FG Loss: {diceFG_WT_loss}\")\n",
    "print(\"Dice Loss: \", dice_loss)\n",
    "print(f\"Soft Dice Loss: {softdice_loss}\")\n",
    "print(\"Monai Dice Loss: \", monai_dice_loss)\n",
    "print(\"Dice Loss per Class: \", dice_loss_p_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softdice_ET: 0.04545434191823006\n",
      "softdice_TC: 0.43037867546081543\n",
      "softdice_WT: 0.5789464116096497\n",
      "softdice_ET: nan\n",
      "softdice_TC: nan\n",
      "softdice_WT: nan\n"
     ]
    }
   ],
   "source": [
    "# Soft Dice Loss on Brats Regions\n",
    "masks_ET = (masks == 3).unsqueeze(0).unsqueeze(0)\n",
    "masks_TC = (masks == 1) | (masks == 3).unsqueeze(0).unsqueeze(0)\n",
    "masks_WT = (masks > 0).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "probs_ET  = probs[:, 3].unsqueeze(0)\n",
    "probs_TC  = torch.maximum(probs[:, 1], probs[:, 3]).unsqueeze(0)\n",
    "probs_WT  = torch.maximum(probs_TC.squeeze(0), probs[:, 2]).unsqueeze(0)\n",
    "\n",
    "softdiceL_ET = SoftDiceLoss(probs_ET, masks_ET)\n",
    "softdiceL_TC = SoftDiceLoss(probs_TC, masks_TC)\n",
    "softdiceL_WT = SoftDiceLoss(probs_WT, masks_WT)\n",
    "\n",
    "print(f\"softdice_ET: {softdiceL_ET}\")\n",
    "print(f\"softdice_TC: {softdiceL_TC}\")\n",
    "print(f\"softdice_WT: {softdiceL_WT}\")\n",
    "\n",
    "# Only on Foreground -> nan because our probs are binary\n",
    "softdiceL_ET = SoftDiceLossFG(probs_ET, masks_ET)\n",
    "softdiceL_TC = SoftDiceLossFG(probs_TC, masks_TC)\n",
    "softdiceL_WT = SoftDiceLossFG(probs_WT, masks_WT)\n",
    "\n",
    "print(f\"softdice_ET: {softdiceL_ET}\")\n",
    "print(f\"softdice_TC: {softdiceL_TC}\")\n",
    "print(f\"softdice_WT: {softdiceL_WT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softdiceloss_ET: 0.433322936296463\n",
      "softdiceloss_TC: 0.618174135684967\n",
      "softdiceloss_WT: 0.6826353073120117\n",
      "\n",
      "\n",
      "Foreground softdiceloss_ET: 0.04545434191823006\n",
      "Foreground softdiceloss_TC: 0.43037867546081543\n",
      "Foreground softdiceloss_WT: 0.5789464116096497\n"
     ]
    }
   ],
   "source": [
    "# Soft Dice Loss on Brats Regions with two channels each (0 = BG, 1 = FG)\n",
    "masks_ET = (masks == 3).unsqueeze(0).unsqueeze(0)\n",
    "masks_TC = (masks == 1) | (masks == 3).unsqueeze(0).unsqueeze(0)    \n",
    "masks_WT = (masks > 0).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "probs_ET_FG  = probs[:, 3].unsqueeze(0)\n",
    "probs_ET_BG = torch.maximum(torch.maximum(probs[:, 0], probs[:, 1]), probs[:, 2]).unsqueeze(0)\n",
    "probs_ET = torch.cat((probs_ET_BG, probs_ET_FG), dim=1)\n",
    "\n",
    "probs_TC_FG  = torch.maximum(probs[:, 1], probs[:, 3]).unsqueeze(0)\n",
    "probs_TC_BG = torch.maximum(probs[:, 0], probs[:, 2]).unsqueeze(0)\n",
    "probs_TC = torch.cat((probs_TC_BG, probs_TC_FG), dim=1)\n",
    "\n",
    "probs_WT_FG  = torch.maximum(torch.maximum(probs[: , 1], probs[:, 2]), probs[:, 3]).unsqueeze(0)\n",
    "probs_WT_BG = probs[:, 0].unsqueeze(0)\n",
    "\n",
    "probs_WT = torch.cat((probs_WT_BG, probs_WT_FG), dim=1)\n",
    "\n",
    "softdiceL_ET = SoftDiceLoss(probs_ET, masks_ET)\n",
    "softdiceL_TC = SoftDiceLoss(probs_TC, masks_TC)\n",
    "softdiceL_WT = SoftDiceLoss(probs_WT, masks_WT)\n",
    "\n",
    "print(f\"softdiceloss_ET: {softdiceL_ET}\")\n",
    "print(f\"softdiceloss_TC: {softdiceL_TC}\")\n",
    "print(f\"softdiceloss_WT: {softdiceL_WT}\")\n",
    "\n",
    "# Only on Foreground -> nan because our probs are binary\n",
    "softdiceFGL_ET = SoftDiceLossFG(probs_ET, masks_ET)\n",
    "softdiceFGL_TC = SoftDiceLossFG(probs_TC, masks_TC)\n",
    "softdiceFGL_WT = SoftDiceLossFG(probs_WT, masks_WT)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Foreground softdiceloss_ET: {softdiceFGL_ET}\")\n",
    "print(f\"Foreground softdiceloss_TC: {softdiceFGL_TC}\")\n",
    "print(f\"Foreground softdiceloss_WT: {softdiceFGL_WT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softdiceloss_ET: 0.433322936296463\n",
      "softdiceloss_TC: 0.618174135684967\n",
      "softdiceloss_WT: 0.6826353073120117\n",
      "\n",
      "\n",
      "Foreground softdiceloss_ET: 0.04545434191823006\n",
      "Foreground softdiceloss_TC: 0.43037867546081543\n",
      "Foreground softdiceloss_WT: 0.5789464116096497\n"
     ]
    }
   ],
   "source": [
    "# Soft Dice Loss on Brats Regions with two channels each (0 = BG, 1 = FG) -> doesn't make a difference\n",
    "masks_ET = (masks == 3)\n",
    "masks_TC = (masks == 1) | (masks == 3)\n",
    "masks_WT = (masks > 0)\n",
    "\n",
    "masks_ET_oh = torch.nn.functional.one_hot(masks_ET.long(), 2).permute(2, 0, 1).unsqueeze(0)\n",
    "masks_TC_oh = torch.nn.functional.one_hot(masks_TC.long(), 2).permute(2, 0, 1).unsqueeze(0)\n",
    "masks_WT_oh = torch.nn.functional.one_hot(masks_WT.long(), 2).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "masks_ET.unsqueeze_(0).unsqueeze_(0)\n",
    "masks_TC.unsqueeze_(0).unsqueeze_(0)\n",
    "masks_WT.unsqueeze_(0).unsqueeze_(0)\n",
    "\n",
    "probs_ET_FG  = probs[:, 3].unsqueeze(0)\n",
    "probs_ET_BG = torch.maximum(torch.maximum(probs[:, 0], probs[:, 1]), probs[:, 2]).unsqueeze(0)\n",
    "probs_ET = torch.cat((probs_ET_BG, probs_ET_FG), dim=1)\n",
    "\n",
    "probs_TC_FG  = torch.maximum(probs[:, 1], probs[:, 3]).unsqueeze(0)\n",
    "probs_TC_BG = torch.maximum(probs[:, 0], probs[:, 2]).unsqueeze(0)\n",
    "probs_TC = torch.cat((probs_TC_BG, probs_TC_FG), dim=1)\n",
    "\n",
    "probs_WT_FG  = torch.maximum(torch.maximum(probs[: , 1], probs[:, 2]), probs[:, 3]).unsqueeze(0)\n",
    "probs_WT_BG = probs[:, 0].unsqueeze(0)\n",
    "\n",
    "probs_WT = torch.cat((probs_WT_BG, probs_WT_FG), dim=1)\n",
    "\n",
    "softdiceL_ET = SoftDiceLoss(probs_ET, masks_ET_oh)\n",
    "softdiceL_TC = SoftDiceLoss(probs_TC, masks_TC_oh)\n",
    "softdiceL_WT = SoftDiceLoss(probs_WT, masks_WT_oh)\n",
    "\n",
    "print(f\"softdiceloss_ET: {softdiceL_ET}\")\n",
    "print(f\"softdiceloss_TC: {softdiceL_TC}\")\n",
    "print(f\"softdiceloss_WT: {softdiceL_WT}\")\n",
    "\n",
    "# Only on Foreground -> nan because our probs are binary\n",
    "softdiceFGL_ET = SoftDiceLossFG(probs_ET, masks_ET_oh)\n",
    "softdiceFGL_TC = SoftDiceLossFG(probs_TC, masks_TC_oh)\n",
    "softdiceFGL_WT = SoftDiceLossFG(probs_WT, masks_WT_oh)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Foreground softdiceloss_ET: {softdiceFGL_ET}\")\n",
    "print(f\"Foreground softdiceloss_TC: {softdiceFGL_TC}\")\n",
    "print(f\"Foreground softdiceloss_WT: {softdiceFGL_WT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ChatGPT approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_region_logits(logits, target_classes):\n",
    "    \"\"\"\n",
    "    Efficiently sum the logits for the given target classes.\n",
    "    \"\"\"\n",
    "    return logits[:, target_classes].sum(dim=1, keepdim=True)\n",
    "\n",
    "def prepare_region_gt(gt, target_classes):\n",
    "    \"\"\"\n",
    "    Efficiently create a binary mask for the relevant target classes.\n",
    "    \"\"\"\n",
    "    return torch.isin(gt, torch.tensor(target_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape probs_tc: torch.Size([1, 1, 3, 3])\n",
      "shape gt tc: torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "brats_regions = {'ET': [3], 'TC': [1, 3], 'WT': [1, 2, 3]}\n",
    "\n",
    "probs_tc = prepare_region_logits(probs, brats_regions['TC'])\n",
    "gt_tc = prepare_region_gt(masks.unsqueeze(0).unsqueeze(0), brats_regions['TC'])\n",
    "probs_et = prepare_region_logits(probs, brats_regions['ET'])\n",
    "gt_et = prepare_region_gt(masks.unsqueeze(0).unsqueeze(0), brats_regions['ET'])\n",
    "probs_wt = prepare_region_logits(probs, brats_regions['WT'])\n",
    "gt_wt = prepare_region_gt(masks.unsqueeze(0).unsqueeze(0), brats_regions['WT'])\n",
    "\n",
    "print(f\"shape probs_tc: {probs_tc.shape}\")\n",
    "print(f\"shape gt tc: {gt_tc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Dice Loss ET: 0.04545434191823006\n",
      "Soft Dice Loss TC: 0.4318172037601471\n",
      "Soft Dice Loss WT: 0.5864652395248413\n",
      "Soft Dice FG Loss  ET: nan\n",
      "Soft Dice FG Loss TC: nan\n",
      "Soft Dice FG Loss WT: nan\n"
     ]
    }
   ],
   "source": [
    "softdiceloss_et = SoftDiceLoss(probs_et, gt_et)\n",
    "softdicelossFG_et = SoftDiceLossFG(probs_et, gt_et)\n",
    "\n",
    "softdiceloss_tc = SoftDiceLoss(probs_tc, gt_tc)\n",
    "softdicelossFG_tc = SoftDiceLossFG(probs_tc, gt_tc)\n",
    "\n",
    "softdiceloss_wt = SoftDiceLoss(probs_wt, gt_wt)\n",
    "softdicelossFG_wt = SoftDiceLossFG(probs_wt, gt_wt)\n",
    "\n",
    "print(f\"Soft Dice Loss ET: {softdiceloss_et}\")\n",
    "print(f\"Soft Dice Loss TC: {softdiceloss_tc}\")\n",
    "print(f\"Soft Dice Loss WT: {softdiceloss_wt}\")\n",
    "\n",
    "print(f\"Soft Dice FG Loss  ET: {softdicelossFG_et}\")\n",
    "print(f\"Soft Dice FG Loss TC: {softdicelossFG_tc}\")\n",
    "print(f\"Soft Dice FG Loss WT: {softdicelossFG_wt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
